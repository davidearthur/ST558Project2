---
title: "ST 558 Project 2"
author: "David Arthur"
date: "6/28/2021"
output:
  github_document:
    toc: true
always_allow_html: true
params:
  dayOfWeek: "Saturday"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
library(tidyverse)
library(caret)
library(corrplot)
library(ggplot2)
library(GGally)
library(knitr)
library(faraway)
library(leaps)
```

# Introduction




We begin by reading in the data, changing the names of some factor levels, and filtering by day of week
```{r message = FALSE, warning = FALSE}
day <- readr::read_csv("day.csv", col_types = cols(
  season = col_factor(),
  yr = col_factor(),
  mnth = col_factor(),
  holiday = col_factor(),
  weekday = col_factor(),
  workingday = col_factor(),
  weathersit = col_factor()))

day <- day %>% mutate(season = fct_recode(season, winter = "1", spring = "2", summer = "3", fall = "4")) %>%
  mutate(yr = fct_recode(yr, "2011" = "0", "2012" = "1")) %>%
  mutate(weekday = fct_recode(weekday, Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6")) %>%
  mutate(weathersit = fct_recode(weathersit, clear = "1", mist = "2", lightRainOrSnow = "3", heavyRainOrSnow = "4")) %>%
  filter(weekday == params$dayOfWeek)

# read in version without factors for correlation plot
dayNF <- readr::read_csv("day.csv", col_types = cols(
  weekday = col_factor()))
```

Next, we partition the data into training and test sets
```{r}
set.seed(21)
trainIndex <- createDataPartition(day$cnt, p = 0.7, list = FALSE)
dayTrain <- day[trainIndex, ]
dayTest <- day[-trainIndex, ]
```


# Summarizations
We begin our exploratory analysis of the data with a graphical overview of the relationships between variables.  Obvious patterns in the plots, as well as high correlation values, indicate associations between variables.
```{r message = FALSE}
GGally::ggpairs(dayTrain %>% select(2:6, 8:9, atemp, windspeed, registered, casual))
# dayNFCor <- cor(as.matrix(dayNF %>% select(3:9, atemp, windspeed, casual, registered,cnt)))
# corrplot(dayNFCor, type = "upper", tl.pos = "lt")
# corrplot(dayNFCor, type = "lower", method = "number", add = TRUE, diag = FALSE, tl.pos = "n")
```

We will now look in more detail at relationships between time-related variables and the `registered` response variable. When we do our linear regression modeling we will need to decide which (if any) of these predictors to use. For example, the date variable (`dteday`) and `season` may not be useful in the presence of `weekday`, `mnth`, and `yr` (or vice versa), as they provide largely redundant information.
```{r}
g <- ggplot(data = dayTrain)
g + geom_point(aes(x = dteday, y = registered))

meanByMonthYr <- dayTrain %>% group_by(mnth, yr) %>%
  summarize(meanReg = mean(registered))
g2 <- ggplot(meanByMonthYr, aes(x = mnth))
g2 + geom_bar(aes(y = meanReg, fill = yr), position = "dodge", stat = "identity")
```

We will look next in more detail at the relationship between quantitative weather variables and the `registered` response variable.  The appearance of nonlinear relationships in the plots may indicate the need for quadratic terms in our linear regression models.  The adjusted temperature variable, `atemp`, seems particularly likely to require a quadratic term, as both low and high temperatures can discourage people from bicycling.  Similarly, with humidity and windspeed, low to moderate values may have no effect, but particularly high values could have an effect, so those variables may also require quadratic terms.
```{r}
g + geom_point(aes(x = atemp, y = registered)) + facet_wrap(~ yr)
g + geom_point(aes(x = hum, y = registered)) + facet_wrap(~ yr)
g + geom_point(aes(x = windspeed, y = registered)) + facet_wrap(~ yr)
```

We now view at a table displaying the mean number of `registered`, `casual`, and total riders at each level of the categorical `weathersit` variable.  It seems plausible that in rain or snow, the number of casual riders might decrease by a larger factor than would the number of registered riders.
```{r}
meanByWeather <- dayTrain %>% group_by(weathersit) %>%
  summarize(meanCas = mean(casual), meanReg = mean(registered), meanTotal = mean(cnt))
kable(meanByWeather, digits = 1, col.names = c("Weather", "Mean Casual Riders", "Mean Registered Riders", "Mean Total Riders"), caption = "Average # of riders by weather category")
```

Exploratory data analysis and summary (James)
```{r carr_explore, message=FALSE, fig.height = 9, fig.width = 15}
ggpairs(dayTrain %>% select(-instant,-dteday, -season, -yr, -cnt, -weekday), 
        ggplot2::aes(colour=workingday))

```
Notes from looking at ggpairs plots:
Working days are the highest usage for registered riders, but non-working days are the highest usage for casual riders. Registered riders are the primary volume, so we definitely care most about them but worth keeping in mind. There are two types of non-working days: weekends and holidays, and there is a difference in volume for each of those rider types depending on whether it is a holiday or a weekend. 

Air temperature and temperature are nearly 100% correlated. We should probably figure out which one of them we want to use. Speaking of correlated, can we drop the date and only use months? Unfortunately, it looks like we need to keep the year field as well, since year 2 had better performance than year 1. Do we want to keep season and month? I lean towards keeping year and month, but dropping season and date. Let me know what you think.

Looking at the scatter of casual vs registered, broken out by working day, it's crazy how separate the linear relationships look:
```{r}
g <- ggplot(data=dayTrain, aes(x=registered, y=casual))
g + geom_point(aes(color=workingday))

```
On working days, registered bikes are the main rider group. On non-working days, it switches to casual. Looking at day of the week, we may be able to exclude it since it will be covered by the working day flag and holiday flag, but I guess we can check the models to see if it provides anything extra.

```{r}
g <- ggplot(data=dayTrain %>% 
                 select(weekday, casual, registered) %>%
                 pivot_longer(cols=c(casual, registered),
                              names_to = 'metrics',
                              values_to = 'riders') %>%
                 group_by(weekday, metrics) %>%
                 summarise(avg_riders = mean(riders)), 
            aes(x=weekday, y=avg_riders, fill=metrics))
g + geom_bar(stat='identity', position='dodge')
```
Looking at this graph, weekday definitely seems relatively stable across the days (working days for registered and non-working days for casual are the jumps), but there may be enough variation to include it. 

##I like this graph.  I thought about doing one like it, but wasn't sure how to code it.  pivot_longer hadn't occurred to me.

##About which variables to include, I agree with your comments.  My understanding is that each of us comes up with our own models (I do a linear regression and a random forest, you do a linear regression and a boosted tree), so you and I don't need to include the same predictors.  We do need to agree ahead of time on which response we're going to model (casual, registered, or cnt), so that the results of the 4 models can be compared to each other.  I'm fine with any of the 3.  Do you have a preference?

Yeah, no preference here either. I guess we could just say registered since it's the highest volume customer, and if we were doing this analysis for that company then registered users would be the most important group.

##Sounds good, we'll go with registered.

# Modeling
We will now fit two linear regression models, using differing approaches, with the goal of creating a model that does a good job of predicting the number of registered riders on any given day, based on the values of the predictor variables in the data set.  We will fit the models using the training data set that we partitioned above, and then test the accuracy of the models' predictions using the test data set.

Linear regression estimates the effect of each predictor variable on the mean value of the response variable, with the other predictor variables held constant.  A linear regression model can be expressed as 
$$Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + ... + \beta_pX_{ip} + E_i$$

where $Y_i$ is the response, $i$ represents the observation number, $X_{ij}$ are the predictor variables, and $E_i$ is the normally distributed random error. The $\beta_j$ coefficents must be linear, but the predictor variables can be higher order terms (e.g. $x^2$) or interaction terms (e.g. $x_1x_2$).  Creating a model to estimate the response using observed data, we have
$$\hat{y_i} = \hat\beta_0 + \hat\beta_1x_{i1} + \hat\beta_2x_{i2} + ... + \hat\beta_px_{ip}$$

The $\hat\beta_j$ coefficients (estimates for $\beta_j$) are calculated for each predictor variable to minimize the residual sum of squares, using the observed values of $x_{ij}$ and $y_i$
$$min_{\beta_0, \beta_1, ..., \beta_p}\sum_{i=1}^{n}(y_i - \beta_0 - \beta_1x_{i1} - \beta_2x_{i2} - ... - \beta_px_{ip})^2$$

The linear regression model can be used for inference, to understand the relationships between the predictor variables and the response, as well as for prediction of a mean response given new values of the predictor variables.  There are varying approaches to choosing which predictor variables to include in a linear regression model.  For our first linear regression model, we ....
For our second linear regression model, we ....

###First linear regression model
```{r carr_bestsub, error = TRUE}
library(leaps)

data <- dayTrain %>% 
               filter(weekday == params$dayOfWeek) %>% drop_na() %>%
               select(-instant,-dteday, -season, 
                    -weekday, -atemp, -casual, -cnt)

#this function converts new data to a model matrix
#so that a prediction can be run via matrix multiplication
#on a best subsets model
predict.regsubsets = function(object,newdata,id,...){
      form = as.formula(object$call[[2]]) 
      mat = model.matrix(form,newdata)    
      coefi = coef(object,id=id)          
      xvars = names(coefi)                
      mat[,xvars]%*%coefi               
}


#let's do cross validation with folds
k <- 4
set.seed(21)
folds <- sample(1:k, nrow(data), replace=T)

cv_errors = matrix(NA, k, 16, dimnames = list(NULL, paste(1:16)))

for (j in 1:k) {
  best <- regsubsets(registered ~ ., 
                     data=data[folds!=j,], nvmax=20)
  
  for (i in 1:16) {
    pred <- predict(best, data[folds==j,], id=i)
    
    
    cv_errors[j, i] <- mean((data$registered[folds==j]-pred)^2)
  }
}

# Take the mean of over all folds for each model size
mean_cv_errors = apply(cv_errors, 2, mean)

# Find the model size with the smallest cross-validation error
min = which.min(mean_cv_errors)

#the model w/ 14 variables was best when using 4 fold cv.
#i did 4 fold because there are only about 80 rows of data per weekday

best <- regsubsets(registered ~ ., 
                     data=data, nvmax=20)
coef(best, min)
coef(best, 16)
```

```{r}
fit <- lm(registered ~ temp*hum,
        data=dayTrain %>% 
             filter(weekday == params$dayOfWeek) %>% drop_na() %>%
             select(-instant,-dteday, -season, 
                    -weekday, -atemp, -casual, -cnt))
```

```{r}
names(dayTrain)
```

### Second linear regression model
In this approach, we start with a full linear regression model that includes all of the predictor variables.  We will then reduce collinearity (correlation among predictor variables) by removing redundant predictors until we reach an optimal (lowest) AIC.  We will calculate the condition number ($\kappa$) for each of the candidate models, which is a measure of collinearity.  Roughly, $\kappa < 30$ is considered desirable.  Finally, we will choose among several variations of the optimal model (including various higher order terms) using cross validation.  Cross validation subdivides the training set into 4 (in this case) folds, then fits a model using 3 of those folds, and tests its accuracy predicting on the 4th fold.  This is repeated 3 more times, so that each fold gets a turn as the test set.  Several measures of the performance of the model are returned.  We will choose the best model in terms of lowest Root Mean Squared Error.


First, we view the correlations between the variables.  For days of the week that don't include any holidays, `?` will appear.
```{r arthur_bestMLR, message = FALSE, warning = FALSE}
dayNFCor <- cor(as.matrix(dayNF %>%
                            mutate(weekday = fct_recode(weekday, Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6")) %>%
                            mutate(dteday = as.numeric(dteday)) %>%
                            filter(weekday == params$dayOfWeek) %>%
                            select(2:6, 8:13, registered)))
corrplot(dayNFCor, type = "upper", tl.pos = "lt")
corrplot(dayNFCor, type = "lower", method = "number", add = TRUE, diag = FALSE, tl.pos = "n")
```


```{r}
mlrFull <- lm(registered ~ dteday + season +  yr + mnth + holiday + workingday + 
                weathersit + temp + atemp + hum + windspeed, dayTrain)
sumary(mlrFull)
AIC(mlrFull)
x <- model.matrix(mlrFull)[, -1]
e <- eigen(t(x) %*% x)
# e$val
# condition number =
sqrt(e$val[1]/min(e$val))
```




```{r}
#remove dteday (high collinearity, low p-value)
mlr2 <- update(mlrFull, . ~ . - dteday)
summary(mlr2)
AIC(mlr2)
x <- model.matrix(mlr2)[, -1]
e <- eigen(t(x) %*% x)
# e$val
sqrt(e$val[1]/min(e$val))
#reduced AIC and condition number

#remove temp (high collinearity, low p-value)
mlr3 <- update(mlr2, . ~ . - temp)
summary(mlr3)
AIC(mlr3)
x <- model.matrix(mlr3)[, -1]
e <- eigen(t(x) %*% x)
# e$val
sqrt(e$val[1]/min(e$val))
#reduced AIC and condition number

#remove workingday (high collinearity, low p-value)
mlr4 <- update(mlr3, . ~ . - workingday)
summary(mlr4)
AIC(mlr4)
x <- model.matrix(mlr4)[, -1]
e <- eigen(t(x) %*% x)
# e$val
sqrt(e$val[1]/min(e$val))
#reduced AIC and condition number

#remove season (high collinearity, low p-value)
mlr5 <- update(mlr4, . ~ . - season)
summary(mlr5)
AIC(mlr5)
#don't remove season because AIC increased

#remove month (high collinearity, low p-value)
mlr6 <- update(mlr4, . ~ . - mnth)
summary(mlr6)
AIC(mlr6)
x <- model.matrix(mlr6)[, -1]
e <- eigen(t(x) %*% x)
# e$val
sqrt(e$val[1]/min(e$val))
#reduced AIC and condition number

#remove year from full model instead of dteday (high collinearity)
mlr7 <- update(mlrFull, . ~ . - yr - workingday - temp, dayTrain)
summary(mlr7)
AIC(mlr7)
x <- model.matrix(mlr7)[, -1]
e <- eigen(t(x) %*% x)
# e$val
sqrt(e$val[1]/min(e$val))
#higher AIC than mlr4

#compare to model chosen by leaps::step() function
mlrStep <- step(mlrFull)
names(mlrStep)
mlrStep$call
mlr6$call
AIC(mlr6, mlrStep)
#my choice agrees with step() choice

#diagnostics plot
plot(mlr6$fitted, mlr6$residuals)
#indication of mild nonconstant variance
MASS::boxcox(mlr6)
#Box-Cox lambda close to 1, so no need for transformation of response

#look for nonlinearity with partial residuals plots
termplot(mlr6, partial.resid = TRUE)
#atemp, hum, and windspeed look somewhat nonlinear, so try quadratic terms for them
mlr8 <- update(mlr6, . ~ . + I(atemp^2))
summary(mlr8)
AIC(mlr8)
#improved AIC, so keep atemp^2 in model

mlr9 <- update(mlr8, . ~ . + I(hum^2))
summary(mlr9)
AIC(mlr9)
#improved AIC, so keep hum^2 in model

mlr10 <- update(mlr9, . ~ . + I(windspeed^2))
summary(mlr10)
AIC(mlr10)
#slightly improved AIC, compare using cross validation

#interaction of weather vars w/ holiday seems possible, so try adding to model
mlr11 <- update(mlr9, . ~ . + weathersit:holiday)
summary(mlr11)
AIC(mlr11)
#slightly worse AIC, compare using cross validation
mlr12 <- update(mlr9, . ~ . + atemp:holiday)
summary(mlr12)
AIC(mlr12)
#marginal decrease in AIC, compare using cross validation

#fit best candidate models using cross validation w/ caret package
mlrFit9 <- train(registered ~ season + yr + holiday + weathersit + atemp + hum + windspeed + I(atemp^2) + I(hum^2), data = dayTrain,
    method = "lm",
    preProcess = c("center", "scale"),
    trControl = trainControl(method = "repeatedcv", number = 4, repeats = 3))

mlrFit10 <- train(registered ~ season + yr + holiday + weathersit + atemp + hum + windspeed + I(atemp^2) + I(hum^2)+ I(windspeed^2), data = dayTrain,
    method = "lm",
    preProcess = c("center", "scale"),
    trControl = trainControl(method = "repeatedcv", number = 4, repeats = 3))

mlrFit11 <- train(registered ~ season + yr + holiday + weathersit + atemp + hum + windspeed + I(atemp^2) + I(hum^2) + holiday:weathersit, data = dayTrain,
    method = "lm",
    preProcess = c("center", "scale"),
    trControl = trainControl(method = "repeatedcv", number = 4, repeats = 3))

mlrFit12 <- train(registered ~ season + yr + holiday + weathersit + atemp + hum + windspeed + I(atemp^2) + I(hum^2) + holiday:atemp, data = dayTrain,
    method = "lm",
    preProcess = c("center", "scale"),
    trControl = trainControl(method = "repeatedcv", number = 4, repeats = 3))

names(mlrFit9)
comparison <- data.frame(t(mlrFit9$results), t(mlrFit10$results), t(mlrFit11$results), t(mlrFit12$results))
colnames(comparison) <- c("mlrFit9", "mlrFit10", "mlrFit11", "mlrFit12")
comparison
#The lowest RMSE out of the 4 candidate models varies each time I run cross validation, so I will choose the simplest of the 4, mlrFit9
mlrBest <- mlrFit9

# for potentially automating choice of model
# which.min(c(mlrFit9$results["RMSE"], mlrFit10$results["RMSE"], mlrFit11$results["RMSE"], mlrFit12$results["RMSE"]))
```


```{r random_forest, message = FALSE, warning = FALSE}
rfFit <- train(registered ~ . - instant - casual - cnt, data = dayTrain,
               method = "rf",
               trControl = trainControl(method = "repeatedcv", number = 4, repeats = 3),
               preProcess = c("center", "scale"),
               tuneGrid = expand.grid(mtry = c(2, 7, 10:16, 20, 24)))
rfFit
```

I added an initial version of my MLR and random forest models, and wrote a separate R script (ST558RenderProject2.r) to automate the reports for each day of the week.  There's still plenty of clean-up to do with the output, but the automation generally seems to be working.  Since the dataset now includes only one weekday at a time, some of our graphs, tables, etc. that included weekday don't make as much sense.  I still have some questions about whether we're supposed to do any initial data exploration with the full training set, or if we only work with one day at a time.  I might post a question on the discussion board or go to Wednesday office hours unless it's clear to you.