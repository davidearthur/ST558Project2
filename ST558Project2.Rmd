---
title: "ST 558 Project 2"
author: "David Arthur"
date: "6/28/2021"
output:
  github_document:
    toc: true
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(corrplot)
library(ggplot2)
library(GGally)
library(knitr)
```


Read in data
```{r message = FALSE}
day <- readr::read_csv("day.csv", col_types = cols(
  season = col_factor(),
  yr = col_factor(),
  mnth = col_factor(),
  holiday = col_factor(),
  weekday = col_factor(),
  workingday = col_factor(),
  weathersit = col_factor()))

day <- day %>% mutate(season = fct_recode(season, winter = "1", spring = "2", summer = "3", fall = "4")) %>%
  mutate(yr = fct_recode(yr, "2011" = "0", "2012" = "1")) %>%
  mutate(weekday = fct_recode(weekday, Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6")) %>%
  mutate(weathersit = fct_recode(weathersit, clear = "1", mist = "2", lightRainOrSnow = "3", heavyRainOrSnow = "4")) 

# dayNF <- readr::read_csv("day.csv")
```

Partition data into training and test sets
```{r}
set.seed(21)
trainIndex <- createDataPartition(day$cnt, p = 0.7, list = FALSE)
dayTrain <- day[trainIndex, ]
dayTest <- day[-trainIndex, ]
```

Exploratory data analysis and summary (David)
```{r message = FALSE}
GGally::ggpairs(dayTrain %>% select(3:9, atemp, windspeed, casual, registered, cnt))
# dayNFCor <- cor(as.matrix(dayNF %>% select(3:9, atemp, windspeed, casual, registered,cnt)))
# corrplot(dayNFCor, type = "upper", tl.pos = "lt")
# corrplot(dayNFCor, type = "lower", method = "number", add = TRUE, diag = FALSE, tl.pos = "n")
```

Exploration of individual predictors
```{r}
g <- ggplot(data = dayTrain)
g + geom_point(aes(x = weekday, y = casual))
g + geom_point(aes(x = dteday, y = cnt))

meanByMonth <- dayTrain %>% group_by(mnth) %>%
  summarize(meanCas = mean(casual), meanReg = mean(registered), meanTotal = mean(cnt))
g2 <- ggplot(meanByMonth, aes(x = mnth))
g2 + geom_bar(aes(y = meanCas), stat = "identity")
g2 + geom_bar(aes(y = meanReg), stat = "identity")
g2 + geom_bar(aes(y = meanTotal), stat = "identity")

meanByYear <- dayTrain %>% group_by(yr) %>%
  summarize(meanCas = mean(casual), meanReg = mean(registered), meanTotal = mean(cnt))
g2 <- ggplot(meanByYear, aes(x = yr))
g2 + geom_bar(aes(y = meanCas), stat = "identity")
g2 + geom_bar(aes(y = meanReg), stat = "identity")
g2 + geom_bar(aes(y = meanTotal), stat = "identity")

meanByWeather <- dayTrain %>% group_by(weathersit) %>%
  summarize(meanCas = mean(casual), meanReg = mean(registered), meanTotal = mean(cnt))
kable(meanByWeather, digits = 1, col.names = c("Weather", "Mean Casual Riders", "Mean Registered Riders", "Mean Total Riders"), caption = "Average # of riders by weather category")

meanByHoliday <- dayTrain %>% filter(workingday == 0) %>%
  group_by(holiday) %>%
  summarize(meanCas = mean(casual), meanReg = mean(registered), meanTotal = mean(cnt))
kable(meanByHoliday, digits = 1, col.names = c("Holiday (0 = no, 1 = yes)", "Mean Casual Riders", "Mean Registered Riders", "Mean Total Riders"), caption = "Average # of riders on holidays vs. non-holiday non-workdays")

```

Exploratory data analysis and summary (James)
```{r carr_explore, message=FALSE, fig.height = 9, fig.width = 15}
ggpairs(dayTrain %>% select(-instant,-dteday, -season, -yr, -cnt), 
        ggplot2::aes(colour=workingday))

```
Notes from looking at ggpairs plots:
Working days are the highest usage for registered riders, but non-working days are the highest usage for casual riders. Registered riders are the primary volume, so we definitely care most about them but worth keeping in mind. There are two types of non-working days: weekends and holidays, and there is a difference in volume for each of those rider types depending on whether it is a holiday or a weekend. 

Air temperature and temperature are nearly 100% correlated. We should probably figure out which one of them we want to use. Speaking of correlated, can we drop the date and only use months? Unfortunately, it looks like we need to keep the year field as well, since year 2 had better performance than year 1. Do we want to keep season and month? I lean towards keeping year and month, but dropping season and date. Let me know what you think.

Looking at the scatter of casual vs registered, broken out by working day, it's crazy how separate the linear relationships look:
```{r}
g <- ggplot(data=dayTrain, aes(x=registered, y=casual))
g + geom_point(aes(color=workingday))

```
On working days, registered bikes are the main rider group. On non-working days, it switches to casual. Looking at day of the week, we may be able to exclude it since it will be covered by the working day flag and holiday flag, but I guess we can check the models to see if it provides anything extra.

```{r}
g <- ggplot(data=dayTrain %>% 
                 select(weekday, casual, registered) %>%
                 pivot_longer(cols=c(casual, registered),
                              names_to = 'metrics',
                              values_to = 'riders') %>%
                 group_by(weekday, metrics) %>%
                 summarise(avg_riders = mean(riders)), 
            aes(x=weekday, y=avg_riders, fill=metrics))
g + geom_bar(stat='identity', position='dodge')
```
Looking at this graph, weekday definitely seems relatively stable across the days (working days for registered and non-working days for casual are the jumps), but there may be enough variation to include it. 

##I like this graph.  I thought about doing one like it, but wasn't sure how to code it.  pivot_longer hadn't occurred to me.

##About which variables to include, I agree with your comments.  My understanding is that each of us comes up with our own models (I do a linear regression and a random forest, you do a linear regression and a boosted tree), so you and I don't need to include the same predictors.  We do need to agree ahead of time on which response we're going to model (casual, registered, or cnt), so that the results of the 4 models can be compared to each other.  I'm fine with any of the 3.  Do you have a preference?

Yeah, no preference here either. I guess we could just say registered since it's the highest volume customer, and if we were doing this analysis for that company then registered users would be the most important group.


```{r carr_bestsub}
library(leaps)

data <- dayTrain %>% 
               filter(weekday == 1) %>% drop_na() %>%
               select(-instant,-dteday, -season, 
                    -weekday, -atemp, -casual, -cnt)

#this function converts new data to a model matrix
#so that a prediction can be run via matrix multiplication
#on a best subsets model
predict.regsubsets = function(object,newdata,id,...){
      form = as.formula(object$call[[2]]) 
      mat = model.matrix(form,newdata)    
      coefi = coef(object,id=id)          
      xvars = names(coefi)                
      mat[,xvars]%*%coefi               
}


#let's do cross validation with folds
k <- 4
set.seed(21)
folds <- sample(1:k, nrow(data), replace=T)

cv_errors = matrix(NA, k, 16, dimnames = list(NULL, paste(1:16)))

for (j in 1:k) {
  best <- regsubsets(registered ~ ., 
                     data=data[folds!=j,], nvmax=20)
  
  for (i in 1:16) {
    pred <- predict(best, data[folds==j,], id=i)
    
    
    cv_errors[j, i] <- mean((temp_data$registered[folds==j]-pred)^2)
  }
}

# Take the mean of over all folds for each model size
mean_cv_errors = apply(cv_errors, 2, mean)

# Find the model size with the smallest cross-validation error
min = which.min(mean_cv_errors)

#the model w/ 14 variables was best when using 4 fold cv.
#i did 4 fold because there are only about 80 rows of data per weekday

best_full <- regsubsets(registered ~ ., 
                     data=temp_data[folds!=j,], nvmax=20)

```

```{r}
fit <- lm(registered ~ temp*hum,
        data=dayTrain %>% 
             filter(weekday == 1) %>% drop_na() %>%
             select(-instant,-dteday, -season, 
                    -weekday, -atemp, -casual, -cnt))
```


